{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Dataset Exploration & Modeling\n",
    "\n",
    "This notebook performs exploratory data analysis (EDA), feature engineering, and builds machine learning models to predict survival (target variable `Survived`) using the provided Titanic dataset CSV file (`Titanic-Dataset.csv`).\n",
    "\n",
    "## Outline\n",
    "1. Setup & Data Loading\n",
    "2. Quick Data Overview\n",
    "3. Exploratory Data Analysis (EDA)\n",
    "4. Feature Engineering\n",
    "5. Preprocessing Pipelines\n",
    "6. Baseline & Advanced Models\n",
    "7. Model Evaluation & Comparison\n",
    "8. Feature Importance & Interpretability\n",
    "9. Save Trained Model (Optional)\n",
    "\n",
    "You can execute cells sequentially. Adjust or extend as needed."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. Imports & Settings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "pd.options.display.max_columns = 100\n",
    "DATA_PATH = Path('.')  # Adjust if needed\n",
    "CSV_FILE = DATA_PATH / 'Titanic-Dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2. Load Data\n",
    "df = pd.read_csv(CSV_FILE)\n",
    "print(f\"Shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Observations\n",
    "- Target: `Survived` (0 = No, 1 = Yes)\n",
    "- Categorical features include: `Gender`, `Embarked`, `Cabin`, `Ticket`\n",
    "- Potential to extract signal from `Name` (Title), `Ticket` (group size), `Cabin` (Deck), family relations (`SibSp`, `Parch`).\n",
    "- Missing values expected in `Age`, `Cabin`, possibly `Embarked`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3. Basic Info & Missingness\n",
    "display(df.info())\n",
    "display(df.describe(include='number').T)\n",
    "display(df.describe(include='object').T)\n",
    "\n",
    "missing = df.isna().mean().sort_values(ascending=False)\n",
    "print('Missing value ratio:')\n",
    "missing.to_frame('missing_ratio').head(15)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 4. Target Distribution\n",
    "survival_rate = df['Survived'].mean()\n",
    "print(f\"Overall survival rate: {survival_rate:.2%}\")\n",
    "sns.countplot(data=df, x='Survived')\n",
    "plt.title('Survival Count')\n",
    "plt.show()\n",
    "\n",
    "sns.barplot(x='Gender', y='Survived', data=df)\n",
    "plt.title('Survival Rate by Gender')\n",
    "plt.show()\n",
    "\n",
    "sns.barplot(x='Pclass', y='Survived', data=df)\n",
    "plt.title('Survival Rate by Passenger Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Variables vs Survival\n",
    "We inspect distributions and potential separability of Age and Fare by survival outcome."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12,4))\n",
    "sns.kdeplot(data=df, x='Age', hue='Survived', common_norm=False, ax=axes[0])\n",
    "axes[0].set_title('Age Distribution by Survival')\n",
    "sns.kdeplot(data=df[df['Fare'] < 200], x='Fare', hue='Survived', common_norm=False, ax=axes[1])\n",
    "axes[1].set_title('Fare (<200) Distribution by Survival')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Heatmap\n",
    "Note: We'll engineer additional features later; initial correlation of raw numeric features shown below."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "numeric_cols = ['Survived','Age','SibSp','Parch','Fare','Pclass']\n",
    "corr = df[numeric_cols].corr()\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Heatmap (Raw Numeric)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering\n",
    "We will create the following engineered features:\n",
    "- `Title` extracted from `Name`\n",
    "- Group rare titles into 'Rare'\n",
    "- `FamilySize = SibSp + Parch + 1`\n",
    "- `IsAlone = 1 if FamilySize == 1 else 0`\n",
    "- `TicketGroupSize` (count of same Ticket)\n",
    "- `Deck` extracted from Cabin (first letter) with missing as 'M'\n",
    "- `HasCabin` binary flag\n",
    "- `FarePerPerson = Fare / FamilySize`\n",
    "- `AgeBucket` (binned Age)\n",
    "- Interaction: `Age*Class`\n",
    "- Optionally categorize Ticket prefix\n",
    "\n",
    "We'll keep raw columns for reference but pass only selected ones to the model pipeline."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extract_title(name: str):\n",
    "    match = re.search(r',\\s*([^\\.]+)\\.', name)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return 'Unknown'\n",
    "\n",
    "df['Title'] = df['Name'].apply(extract_title)\n",
    "\n",
    "# Map rare titles\n",
    "title_counts = df['Title'].value_counts()\n",
    "rare_titles = title_counts[title_counts < 10].index\n",
    "df['Title'] = df['Title'].replace(rare_titles, 'Rare')\n",
    "\n",
    "# Family features\n",
    "df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "df['IsAlone'] = (df['FamilySize'] == 1).astype(int)\n",
    "\n",
    "# Ticket group size\n",
    "ticket_counts = df['Ticket'].value_counts()\n",
    "df['TicketGroupSize'] = df['Ticket'].map(ticket_counts)\n",
    "\n",
    "# Deck extraction\n",
    "def extract_deck(cabin):\n",
    "    if pd.isna(cabin) or cabin.strip() == '':\n",
    "        return 'M'  # Missing\n",
    "    return cabin.strip()[0]\n",
    "\n",
    "df['Deck'] = df['Cabin'].apply(extract_deck)\n",
    "df['HasCabin'] = (~df['Cabin'].isna() & (df['Cabin'].str.strip() != '')).astype(int)\n",
    "\n",
    "# Fare per person\n",
    "df['FarePerPerson'] = df['Fare'] / df['FamilySize']\n",
    "\n",
    "# Age bucket (temporary - will impute age first; this creates NaNs if Age is NaN)\n",
    "df['AgeBucket'] = pd.cut(df['Age'], bins=[0,5,12,18,30,45,60,80], right=False)\n",
    "\n",
    "# Interaction\n",
    "df['Age*Class'] = df['Age'] * df['Pclass']\n",
    "\n",
    "# Ticket prefix\n",
    "def ticket_prefix(t):\n",
    "    t = str(t)\n",
    "    t = t.replace('.', '').replace('/', '').strip()\n",
    "    parts = t.split()\n",
    "    if parts and not parts[0].isdigit():\n",
    "        return parts[0]\n",
    "    return 'NOPREFIX'\n",
    "df['TicketPrefix'] = df['Ticket'].apply(ticket_prefix)\n",
    "\n",
    "print('Engineered columns added.')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Engineered Features vs Survival"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,3, figsize=(15,8))\n",
    "sns.barplot(x='Title', y='Survived', data=df, ax=axes[0,0])\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "sns.barplot(x='Deck', y='Survived', data=df, ax=axes[0,1])\n",
    "sns.barplot(x='IsAlone', y='Survived', data=df, ax=axes[0,2])\n",
    "sns.barplot(x='FamilySize', y='Survived', data=df, ax=axes[1,0])\n",
    "sns.barplot(x='TicketGroupSize', y='Survived', data=df, ax=axes[1,1])\n",
    "sns.barplot(x='HasCabin', y='Survived', data=df, ax=axes[1,2])\n",
    "axes[1,0].set_xticklabels(axes[1,0].get_xticklabels(), rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Modeling\n",
    "We'll build a preprocessing pipeline using `ColumnTransformer` and test multiple models:\n",
    "- Logistic Regression (baseline)\n",
    "- Random Forest\n",
    "- Gradient Boosting (e.g., XGBoost/LightGBM optional if installed)\n",
    "- Extra Trees\n",
    "\n",
    "We will:\n",
    "1. Split features & target\n",
    "2. Define numeric & categorical columns\n",
    "3. Create pipelines with imputation, scaling, one-hot encoding\n",
    "4. Cross-validate models\n",
    "5. Compare metrics (Accuracy, ROC AUC, F1)\n",
    "\n",
    "Note: For reproducibility, we set random_state."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "\n",
    "# Feature selection for modeling\n",
    "target = 'Survived'\n",
    "feature_cols = [\n",
    "    'Pclass','Gender','Age','SibSp','Parch','Fare','Embarked',\n",
    "    'Title','FamilySize','IsAlone','TicketGroupSize','Deck','HasCabin',\n",
    "    'FarePerPerson','Age*Class','TicketPrefix'\n",
    "]\n",
    "\n",
    "X = df[feature_cols].copy()\n",
    "y = df[target].copy()\n",
    "\n",
    "numeric_features = ['Age','SibSp','Parch','Fare','FamilySize','TicketGroupSize','FarePerPerson','Age*Class','Pclass']\n",
    "categorical_features = ['Gender','Embarked','Title','Deck','IsAlone','HasCabin','TicketPrefix']\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "models = {\n",
    "    'LogReg': LogisticRegression(max_iter=1000, C=1.0, random_state=42),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=400, max_depth=None, random_state=42),\n",
    "    'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
    "    'ExtraTrees': ExtraTreesClassifier(n_estimators=500, random_state=42)\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    pipe = Pipeline(steps=[('prep', preprocessor), ('clf', model)])\n",
    "    scores = cross_validate(pipe, X, y, cv=cv, scoring=['accuracy','roc_auc','f1'], n_jobs=-1)\n",
    "    results.append({\n",
    "        'model': name,\n",
    "        'accuracy_mean': scores['test_accuracy'].mean(),\n",
    "        'roc_auc_mean': scores['test_roc_auc'].mean(),\n",
    "        'f1_mean': scores['test_f1'].mean()\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(by='roc_auc_mean', ascending=False)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection\n",
    "Pick the top-performing model (e.g., highest ROC AUC). We'll refit it on the full dataset and inspect feature importances (for tree-based models) or coefficients (for Logistic Regression)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Choose best model by ROC AUC\n",
    "best_model_name = results_df.iloc[0]['model']\n",
    "best_model = models[best_model_name]\n",
    "print(f\"Best model selected: {best_model_name}\")\n",
    "\n",
    "final_pipeline = Pipeline(steps=[('prep', preprocessor), ('clf', best_model)])\n",
    "final_pipeline.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance / Coefficients\n",
    "We extract the processed feature names and show importance for tree models or coefficients for logistic regression. Note that after one-hot encoding, features expand."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_feature_names(preprocessor):\n",
    "    num_feats = numeric_features\n",
    "    cat_pipeline = preprocessor.named_transformers_['cat']\n",
    "    ohe = cat_pipeline.named_steps['onehot']\n",
    "    cat_ohe_feats = ohe.get_feature_names_out(categorical_features)\n",
    "    return list(num_feats) + list(cat_ohe_feats)\n",
    "\n",
    "feature_names = get_feature_names(preprocessor)\n",
    "\n",
    "importances_df = None\n",
    "clf = final_pipeline.named_steps['clf']\n",
    "if hasattr(clf, 'feature_importances_'):\n",
    "    importances_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': clf.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "elif hasattr(clf, 'coef_'):\n",
    "    coef = clf.coef_[0]\n",
    "    importances_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': coef\n",
    "    }).assign(abs_importance=lambda d: d['importance'].abs()).sort_values('abs_importance', ascending=False)\n",
    "\n",
    "importances_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot top 20 importances if tree-based\n",
    "if hasattr(clf, 'feature_importances_'):\n",
    "    top_imp = importances_df.head(20)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.barplot(y='feature', x='importance', data=top_imp)\n",
    "    plt.title(f'Top 20 Feature Importances ({best_model_name})')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "elif hasattr(clf, 'coef_'):\n",
    "    top_coef = importances_df.head(20)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.barplot(y='feature', x='importance', data=top_coef)\n",
    "    plt.title(f'Top Coefficients ({best_model_name})')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Optional: Hyperparameter Tuning (Example for RandomForest)\n",
    "This cell demonstrates a small grid search. You can expand the parameter grid if you want deeper optimization (note: increases runtime)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'clf__n_estimators': [200, 400],\n",
    "    'clf__max_depth': [None, 6, 10],\n",
    "    'clf__min_samples_split': [2, 5]\n",
    "} if best_model_name == 'RandomForest' else None\n",
    "\n",
    "if param_grid:\n",
    "    tuning_pipeline = Pipeline(steps=[('prep', preprocessor), ('clf', RandomForestClassifier(random_state=42))])\n",
    "    grid = GridSearchCV(tuning_pipeline, param_grid=param_grid, scoring='roc_auc', cv=3, n_jobs=-1)\n",
    "    grid.fit(X, y)\n",
    "    print('Best params:', grid.best_params_)\n",
    "    print('Best ROC AUC:', grid.best_score_)\n",
    "else:\n",
    "    print('Skipping RF tuning (best model is not RandomForest).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Simple Prediction Function\n",
    "Utility to predict survival probability for a small sample (manually crafted or subset)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict_samples(sample_df: pd.DataFrame):\n",
    "    return final_pipeline.predict_proba(sample_df)[:,1]\n",
    "\n",
    "# Example: use first 5 passengers (dropping target)\n",
    "sample = X.head(5)\n",
    "probs = predict_samples(sample)\n",
    "pd.DataFrame({'PassengerIndex': sample.index, 'Predicted_Survival_Prob': probs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. (Optional) Persist the Final Model\n",
    "Uncomment the code below to save the pipeline for later use (requires joblib)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from joblib import dump\n",
    "# dump(final_pipeline, 'titanic_model_pipeline.joblib')\n",
    "# print('Model saved to titanic_model_pipeline.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Next Steps / Ideas\n",
    "- Try alternative algorithms (e.g., XGBoost, LightGBM, CatBoost)\n",
    "- Use cross-validation with stratified repeated splits\n",
    "- Calibrate probabilities (CalibratedClassifierCV)\n",
    "- Perform feature selection or SHAP analysis for interpretability\n",
    "- Evaluate with additional metrics (precision-recall curves)\n",
    "- Handle potential leakage / optimize engineered features\n",
    "\n",
    "This notebook provides a solid baseline for experimentation."
   ]
  }
 ]
}